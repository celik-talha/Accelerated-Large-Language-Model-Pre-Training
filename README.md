# Accelerated-Large-Language-Model-Pre-Training
This project accelerates LLM pre-training by splitting datasets, training different model layers in parallel, and merging sub-models. The approach reduces training time significantly while maintaining strong performance on major benchmarks.
