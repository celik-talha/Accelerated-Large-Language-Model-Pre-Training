{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYYeaYa7oweI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -U transformers datasets accelerate\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri setini y√ºkle\n",
        "dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "# Toplam uzunluk ve √ßeyrek uzunluk\n",
        "total_len = len(train_dataset)\n",
        "quarter_len = total_len // 4\n",
        "\n",
        "train_dataset = train_dataset.select(range(0, 1 * quarter_len))\n",
        "#train_dataset = train_dataset.select(range(1 * quarter_len, 2 * quarter_len))\n",
        "#train_dataset = train_dataset.select(range(2 * quarter_len, 3 * quarter_len))\n",
        "#train_dataset = train_dataset.select(range(3 * quarter_len, 4 * quarter_len))"
      ],
      "metadata": {
        "id": "nI7Ve1Ojo3bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Tokenizer and Model\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"unsloth/Llama-3.2-1B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "eTsuoqxSo4-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocess the data\n",
        "def preprocess(example):\n",
        "    prompt = f\"{example['question']} {example['answer']}\"\n",
        "    return tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)\n",
        "#tokenized_test = test_dataset.map(preprocess, remove_columns=test_dataset.column_names)"
      ],
      "metadata": {
        "id": "mzAXFtNWo6XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create DataLoaders\n",
        "train_loader = DataLoader(tokenized_train, batch_size=8, shuffle=True, collate_fn=lambda x: {\n",
        "    'input_ids': torch.stack([torch.tensor(f['input_ids']) for f in x]),\n",
        "    'attention_mask': torch.stack([torch.tensor(f['attention_mask']) for f in x]),\n",
        "    'labels': torch.stack([torch.tensor(f['input_ids']) for f in x])\n",
        "})\n",
        "\n",
        "#test_loader = DataLoader(tokenized_test, batch_size=8, shuffle=False, collate_fn=lambda x: {\n",
        "#    'input_ids': torch.stack([torch.tensor(f['input_ids']) for f in x]),\n",
        "#    'attention_mask': torch.stack([torch.tensor(f['attention_mask']) for f in x]),\n",
        "#    'labels': torch.stack([torch.tensor(f['input_ids']) for f in x])\n",
        "#})"
      ],
      "metadata": {
        "id": "dfl7Dp_Ro-Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Setup Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
        "print(f\"Total transformer layers: {len(model.model.layers)}\")\n",
        "print(next(model.parameters()).dtype)\n",
        "print(model.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVbeV59RpK8V",
        "outputId": "35c9bf63-55be-4417-d088-3205c6601e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total transformer layers: 16\n",
            "torch.float32\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(model.model.layers):\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "AeBXKoq3tU5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_group = 3\n",
        "\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    if layer_group == 1 and i < 4:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "    elif layer_group == 2 and i >= 4 and i < 8:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "    elif layer_group == 3 and i >= 8 and i < 12:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "    elif layer_group == 4 and i >= 12 and i < 16:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    trainable = any(p.requires_grad for p in layer.parameters())\n",
        "    status = \"‚úÖ\" if trainable else \"‚ùå\"\n",
        "    print(f\"Layer {i}: {status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwL70NmapO6f",
        "outputId": "e606015b-74d6-4629-9dc5-358dd9a186a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: ‚úÖ\n",
            "Layer 1: ‚úÖ\n",
            "Layer 2: ‚úÖ\n",
            "Layer 3: ‚úÖ\n",
            "Layer 4: ‚úÖ\n",
            "Layer 5: ‚úÖ\n",
            "Layer 6: ‚úÖ\n",
            "Layer 7: ‚úÖ\n",
            "Layer 8: ‚úÖ\n",
            "Layer 9: ‚úÖ\n",
            "Layer 10: ‚úÖ\n",
            "Layer 11: ‚úÖ\n",
            "Layer 12: ‚ùå\n",
            "Layer 13: ‚ùå\n",
            "Layer 14: ‚ùå\n",
            "Layer 15: ‚ùå\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(model.model.layers):\n",
        "    if i == 4:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    trainable = any(p.requires_grad for p in layer.parameters())\n",
        "    status = \"‚úÖ\" if trainable else \"‚ùå\"\n",
        "    print(f\"Layer {i}: {status}\")"
      ],
      "metadata": {
        "id": "fmBLUKCppTHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99e6ef0-ac2c-4e4c-db9e-75475bb5595a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0: ‚úÖ\n",
            "Layer 1: ‚úÖ\n",
            "Layer 2: ‚úÖ\n",
            "Layer 3: ‚úÖ\n",
            "Layer 4: ‚úÖ\n",
            "Layer 5: ‚úÖ\n",
            "Layer 6: ‚ùå\n",
            "Layer 7: ‚ùå\n",
            "Layer 8: ‚ùå\n",
            "Layer 9: ‚ùå\n",
            "Layer 10: ‚ùå\n",
            "Layer 11: ‚ùå\n",
            "Layer 12: ‚ùå\n",
            "Layer 13: ‚ùå\n",
            "Layer 14: ‚ùå\n",
            "Layer 15: ‚ùå\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# To Save Steps\n",
        "step_flag = 0\n",
        "\n",
        "# Create gradient scaler\n",
        "scaler = GradScaler()\n",
        "num_epochs = 3\n",
        "j = 0\n",
        "\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "save_steps = [int(total_steps * i / 8) for i in range(1, 8)]\n",
        "current_step = 0  # Toplam adƒ±m sayac\n",
        "\n",
        "# Training loop with mixed precision\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Use autocast for mixed precision\n",
        "        with autocast(device_type=\"cuda\"):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        # Scale loss and call backward\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Unscale gradients and clip them\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Step optimizer and update scaler\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "        # üîí Ara kaydetme noktasƒ±\n",
        "        if current_step in save_steps and step_flag == 1:\n",
        "            j += 1\n",
        "            checkpoint_dir = f\"/content/drive/MyDrive/Bitirme/Models/Gsm8k/Checkpoints/llama-1b-base-gsm8k-4.1.step_{j}\"\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            print(f\"üîí Checkpoint saved at step {current_step}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "final_path = \"/content/drive/MyDrive/Bitirme/Models/Gsm8k/llama-1b-base-gsm8k-9.1\"\n",
        "model.save_pretrained(final_path)\n",
        "tokenizer.save_pretrained(final_path)\n",
        "#torch.save(optimizer.state_dict(), final_path)\n",
        "#torch.save(model.state_dict(), final_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awgcy2rO_yVu",
        "outputId": "9a796a94-7ab9-4d20-89ef-ecd9d828c5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [01:17<00:00,  3.03it/s, Loss=3.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Train Loss: 4.2613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [01:17<00:00,  3.02it/s, Loss=1.94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Train Loss: 2.5911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [01:17<00:00,  3.02it/s, Loss=0.804]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Train Loss: 1.2739\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Bitirme/Models/Gsm8k/llama-1b-base-gsm8k-9.1/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Bitirme/Models/Gsm8k/llama-1b-base-gsm8k-9.1/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Bitirme/Models/Gsm8k/llama-1b-base-gsm8k-9.1/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}